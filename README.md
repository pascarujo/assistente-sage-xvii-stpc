# Assistente Inteligente para Configura√ß√£o do SAGE
Reposit√≥rio com links e material suplementar do trabalho apresentado no XVII STPC. O artigo est√° dispon√≠vel para download no seguinte link: [Artigo STPC](./arquivos/XVII_STPC_IT_Assistente_Sage.pdf).


 ## üöÄ Prot√≥tipo SageLM

 Uma implementa√ß√£o experimental do assistente, batizada de SageLM, est√° dispon√≠vel no seguinte link: [SageLM](https://sagelm.onrender.com).

O assistente foi desenvolvido em python utilizando a biblioteca streamlit e a API do Gemini da Google. Para utiliz√°-lo √© necess√°rio ter uma conta do Google e [gerar uma chave de API do Gemini](https://aistudio.google.com/app/apikey).

Um tutorial de como utilizar o SageLM est√° dispon√≠vel no seguinte link: [Tutorial SageLM](./arquivos/Tutorial_SageLM.pdf).

## üîé Avalia√ß√£o dos especialistas

A pesquisa com especialistas em Sage foi realizada com 13 profissionais. Cada um avaliou pares de perguntas e respectivas respostas geradas pelo Gemini 1.5 Pro. Os especialistas foram instru√≠dos a avaliar a qualidade das respostas do Gemini em termos de corretude, clareza e relev√¢ncia, dando uma nota de 1 a 10 para cada uma. Para o caso de respostas que n√£o atendiam aos crit√©rios estabelecidos, os especialistas foram instru√≠dos a fazer coment√°rios para complementar ou indicar quais seriam as respostas corretas.

As respostas coletadas est√£o dispon√≠veis em [arquivos/pesquisa.xlsx](./arquivos/pesquisa.xlsx).


## üìö Dados de treinamento

Os pares de perguntas e respostas avaliados pelos especialistas foram amostrados da base completa de 1740 perguntas e respostas geradas com o Gemini 1.5 Pro cobrindo assuntos relacionados √† configura√ß√£o de base fonte SCADA do SAGE, incluindo configura√ß√µes espec√≠ficas para IEC 61850, ICCP e DNP. Esta base sofreu ajustes ap√≥s a avalia√ß√£o dos especialistas e est√° dispon√≠vel como um lista json em [arquivos/qa.json](./arquivos/qa.json).

O conjunto de perguntas e respostas tamb√©m est√° dispon√≠vel em formato de treinamento para o GPT no Hugging Face em [pascarujo/sage-qa-training/](https://huggingface.co/datasets/pascarujo/sage-qa-training).


## üß† Treinando o gpt-4o

Voc√™ deve logar na √°rea de API da OpenAI e selecionar [Fine-Tuning](https://platform.openai.com/finetune/) no Dashboard. Clique em Create no canto superior direito e selecione o modelo gpt-4o-mini ou outro do seu interesse. Carregue o [arquivo de treinamento](https://huggingface.co/datasets/pascarujo/sage-qa-training), escolha um identificador para o fine-tuning em `Suffix` e selecione as op√ß√µes de configura√ß√£o para criar o fine-tuning:

- **Seed**: n√∫mero aleat√≥rio para inicializar o algoritmo de treinamento. Especificar o mesmo valor de seed e dos outros par√¢metros de configura√ß√£o facilita a reprodu√ß√£o dos resultados.
- **Batch size**: n√∫mero de exemplos por lote de treinamento.
- **Learning rate multiplier**: multiplicador da taxa de aprendizado.
- **Number of epochs**: n√∫mero de √©pocas de treinamento, ou seja, quantas vezes o algoritmo ver√° todo o conjunto de treinamento.

Voc√™ pode deixar em branco os campos acima e utilizar os valores padr√£o, mas √© recomendado testar diferentes configura√ß√µes para encontrar a que apresenta melhor desempenho. Voc√™ pode ler mais sobre os par√¢metros de configura√ß√£o [aqui](https://platform.openai.com/docs/guides/fine-tuning/).

Ap√≥s preencher os campos, clique em `Create`.

Um dos testes de fine-tuning realizados com o gpt-4o-mini utilizou os seguintes par√¢metros:

- Batch size: 1
- Learning rate multiplier: 0.1
- Number of epochs: 4
- Seed: 132787610

Usando a base de treinamento, o fine-tuning levou aproximadamente 4,5 hora para ser conclu√≠do, treinando um total de `1.542.188` tokens. Observe que existe um custo para treinamento de modelos, voc√™ pode ter mais detalhes [aqui](https://openai.com/api/pricing/).

## üß† Treinando o Gemini

Voc√™ deve entrar no Google AI Studio e selecionar [New Tuned Model](https://aistudio.google.com/fine-tuning) no menu lateral esquerdo. Em Select data for tuning, carregue o [arquivo xlsx de treinamento](./arquivos/qa.xlsx) selecionando `Importar > Upload`. Ap√≥s o arquivo ser carregado, voc√™ precisar√° selecionar quais colunas do arquivo cont√©m as perguntas e respostas para treinamento, conforme a imagem abaixo:

![gemini-fine-tuning-01](https://github.com/user-attachments/assets/5bb56520-a871-4c47-b021-a46aa307c912)

Marque a op√ß√£o `Use first row as header` e clique em `Importar`.
Ap√≥s a importa√ß√£o do treinamento, defina os demais par√¢metros de configura√ß√£o:

- Tuned model name: nome do modelo treinado.
- Choose base model: selecione o modelo base para o treinamento (gemini-1.5-flash-tuning foi o usado neste trabalho).
- Em Advanced settings, voc√™ pode alterar os par√¢metros de configura√ß√£o do treinamento, similar ao que foi feito para o gpt-4o-mini, mas √© poss√≠vel deixar com os valores padr√£o. Clique em `Tune` para iniciar o treinamento.

Um dos testes de fine-tuning realizados com o Gemini 1.5 Flash utilizou os seguintes par√¢metros:

- Choose base model: `gemini-1.5-flash-tuning`
- Batch size: 8
- Learning rate multiplier: 0.0002 
- Number of epochs: 4

Saiba mais sobre o fine-tuning no Gemini [aqui](https://ai.google.dev/gemini-api/docs/model-tuning/).

 
## üêë Treinando o Llama 3.1 8B

O modelo Llama 3.1 8B √© um modelo open source da Meta baseado em LlamA 3.1, mas com apenas 8 bilh√µes de par√¢metros. Ele pode ser treinado utilizando diversas plataformas e m√©todos, mas neste trabalho foi usado a biblioteca [Unsloth](https://github.com/unslothai/unsloth).

O notebook usado para o treinamento est√° em [notebooks/sage_llama.ipynb](https://github.com/pascarujo/assistente-sage-xvii-stpc/blob/main/notebooks/sage_llama.ipynb)

O modelo treinado est√° dispon√≠vel no Hugging Face em [pascarujo/SageLlama-3.1-8B-GGUF](https://huggingface.co/pascarujo/SageLlama-3.1-8B-GGUF).


## ‚≠ê Assistente utilizando o Gemini

Um c√≥digo de teste para utilizar o Gemini 1.5 Flash como um assistente de configura√ß√£o do Sage est√° dispon√≠vel em [notebooks/assistente_gemini.ipynb](https://github.com/pascarujo/assistente-sage-xvii-stpc/blob/main/notebooks/assistente_gemini.ipynb).

Para utilizar o c√≥digo, √© necess√°rio:

- [Gerar uma chave de API do Gemini](https://aistudio.google.com/app/apikey).
- Definir os arquivos que ser√£o utilizados como base de conhecimento para in-context learning. Por padr√£o, os arquivos devem ficar na pasta dados. Como teste, voc√™ pode usar os exemplos de perguntas e respostas dispon√≠veis em [arquivos/qa.xlsx](./arquivos/qa.xlsx).
- Definir o prompt de instru√ß√£o que ser√° utilizado para o modelo. Um exemplo simples est√° dispon√≠vel no notebook.

Observe que esta √© uma vers√£o simplificada do assistente. 

## üìß Exemplos de uso do assistente

Por quest√£o de espa√ßo, apenas alguns exemplos de conversa√ß√£o com o assistente foram inseridos no artigo. No diret√≥rio exemplos est√° o resultado de diversos testes de intera√ß√£o com o prot√≥tipo:

- [Exemplo 01 - Erro de digita√ß√£o no ID do CGS](./exemplos/exemplo_01_erro_digita√ß√£o.md)
- [Exemplo 02 - Ajuda na configura√ß√£o do OPMSK](./exemplos/exemplo_02_opmsk.md)
- [Exemplo 03 - Ajuda para clonar uma base](./exemplos/exemplo_03_duplica√ß√£o_de_pontos.md)
- [Exemplo 04 - Tirando d√∫vidas de um iniciante](./exemplos/exemplo_04_iniciante.md)
- [Exemplo 05 - Descrevendo tabelas](./exemplos/exemplo_05_descri√ß√£o_tabelas.md)
- [Exemplo 06 - Criando um diagrama](./exemplos/exemplo_06_criar_diagrama.md)
- [Exemplo 07 - Resposta a pergunta da base de treinamento](./exemplos/exemplo_07_melhora_na_resposta.md)
- [Exemplo 08 - Vis√£o geral do Sage](./exemplos/exemplo_08_visao_geral.md)
- [Exemplo 09 - Teste de alinhamento](./exemplos/exemplo_09_alinhamento.md)
- [Exemplo 10 - Explica√ß√£o l√∫dica](./exemplos/exemplo_10_l√∫dico.md)
- [Exemplo 11 - Conhecimento especialista](./exemplos/exemplo_11_conhecimento_especialista.md)
- [Exemplo 12 - Rede de difus√£o confi√°vel](./exemplos/exemplo_12_difusao_confiavel.md)

## üìñ Refer√™ncias

- [Attention is all you need](https://arxiv.org/abs/1706.03762)
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
- [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685v1)
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
- [A Survey on Knowledge Graphs: Representation, Acquisition, and Applications](https://arxiv.org/abs/2002.00388v2)
- [From Local to Global: A Graph RAG Approach to Query-Focused Summarization](https://arxiv.org/abs/2404.16130)
- [A modular graph-based Retrieval-Augmented Generation (RAG) system](https://github.com/microsoft/graphrag)
- [On the Opportunities and Risks of Foundation Models](https://arxiv.org/abs/2108.07258)


